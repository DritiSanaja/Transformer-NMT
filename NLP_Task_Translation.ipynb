{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxwBpSMNWubB"
      },
      "outputs": [],
      "source": [
        "#Necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import collections\n",
        "import os\n",
        "import re\n",
        "import keras.backend as K\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjQPBBc4J4TG"
      },
      "source": [
        "Transformation Model for Creating A Translation Machine from English to German. This model was trained using the EUROPARL Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWrp-N9sBOas",
        "outputId": "6d5c243b-8c49-48a7-a4b7-814ab8277eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Here I mounted the Google Drive to my colab environment since I saved the dataset on drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tycuxjbmUJTV"
      },
      "outputs": [],
      "source": [
        "#I define the directory path and file path for english and german text files\n",
        "dir_path = '/content/drive/MyDrive/Colab Notebooks/de-en'\n",
        "en_file_path = os.path.join(dir_path, \"europarl-v7.de-en.en\")\n",
        "de_file_path = os.path.join(dir_path, \"europarl-v7.de-en.de\")\n",
        "\n",
        "#read both files line by line(english and german file)\n",
        "with open(en_file_path, 'r', encoding='utf-8') as f:\n",
        "    english_lines = f.read().splitlines()\n",
        "\n",
        "with open(de_file_path, 'r', encoding='utf-8') as f:\n",
        "    german_lines = f.read().splitlines()\n",
        "\n",
        "# Convert to pandas dataframes and add start and end tokens\n",
        "df_en = pd.DataFrame(english_lines, columns=[\"English words/sentences\"])\n",
        "df_en[\"English words/sentences\"]=(\"<SOS> \"+df_en[\"English words/sentences\"]+\" <EOS>\")\n",
        "\n",
        "df_de = pd.DataFrame(german_lines, columns=[\"German words/sentences\"])\n",
        "df_de[\"German words/sentences\"]=(\"<SOS> \"+df_de[\"German words/sentences\"]+\" <EOS>\")\n",
        "\n",
        "df = pd.concat([df_en, df_de], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZtbvQl1NV4vV"
      },
      "outputs": [],
      "source": [
        "df[\"German word numbers\"]=(df['English words/sentences'].str.split().apply(len))\n",
        "df[\"English word numbers\"]=(df['German words/sentences'].str.split().apply(len))\n",
        "\n",
        "eng = df['English words/sentences']\n",
        "de = df['German words/sentences']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppVu2vxKWmvP",
        "outputId": "0a9f3d1e-5bbb-40af-e243-93f54ed75a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51722761 English words.\n",
            "295399 unique English words.\n",
            "10 Most common words in the English dataset:\n",
            "\"the\" \"<SOS>\" \"<EOS>\" \"of\" \"to\" \"and\" \"in\" \"that\" \"a\" \"is\"\n",
            "\n",
            "48454703 German words.\n",
            "639032 unique German words.\n",
            "10 Most common words in the German dataset:\n",
            "\"<SOS>\" \"<EOS>\" \"die\" \"der\" \"und\" \"in\" \"zu\" \"den\" \"f√ºr\" \"von\"\n"
          ]
        }
      ],
      "source": [
        "english_words_counter = collections.Counter([word for sentence in eng for word in sentence.split()])\n",
        "german_words_counter = collections.Counter([word for sentence in de for word in sentence.split()])\n",
        "\n",
        "print('{} English words.'.format(len([word for sentence in eng for word in sentence.split()])))\n",
        "print('{} unique English words.'.format(len(english_words_counter)))\n",
        "print('10 Most common words in the English dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
        "print()\n",
        "print('{} German words.'.format(len([word for sentence in de for word in sentence.split()])))\n",
        "print('{} unique German words.'.format(len(german_words_counter)))\n",
        "print('10 Most common words in the German dataset:')\n",
        "print('\"' + '\" \"'.join(list(zip(*german_words_counter.most_common(10)))[0]) + '\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TRy9r7W_C6Fa"
      },
      "outputs": [],
      "source": [
        "#in this function I initialized the tokenizer, fitted the tokenzier on the texts(Creating a word index) and then convert thet texts to sequences of integers\n",
        "\n",
        "def tokenize(x):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer\n",
        "\n",
        "def pad(x, length=14):\n",
        "    # If length is None, the function determines the maximum length of the sequences\n",
        "    if length is None:\n",
        "        length = max([len(sentence) for sentence in x])\n",
        "    # Pad the sequences to the specified length with 'post' padding (adding padding at the end)\n",
        "    return pad_sequences(x, maxlen=length, padding='post')\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_texts = []\n",
        "    for sent in text:\n",
        "        # Remove all characters that are not letters, digits, or whitespace\n",
        "        cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', sent)\n",
        "        cleaned_texts.append(cleaned_text)\n",
        "    return cleaned_texts\n",
        "\n",
        "def preprocess(x, y):\n",
        "\n",
        "    # Tokenize the English text and german\n",
        "    preprocess_x, x_tk = tokenize(x)\n",
        "    preprocess_y, y_tk = tokenize(y)\n",
        "\n",
        "    # Pad the tokenized English text and German text to a uniform length\n",
        "    preprocess_x = pad(preprocess_x)\n",
        "    preprocess_y = pad(preprocess_y)\n",
        "\n",
        "    # Return the preprocessed and padded sequences along with the tokenizers\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQiaMTmmXKpZ"
      },
      "outputs": [],
      "source": [
        "#preprocessing\n",
        "preproc_english_sentences, preproc_german_sentences, english_tokenizer, german_tokenizer = preprocess(eng, de)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1dRNaB4DBKJ"
      },
      "outputs": [],
      "source": [
        "#the class positioinal encoding is a custom TensorFlow Keras layer that adds positional information to the input embeddings using the sinusoidal functions\n",
        "#by adding positional encodings, the model gains information about the order of the tokens in the sequence\n",
        "class positional_encoding(tf.keras.layers.Layer):\n",
        "    def __init__(self,max_sentence_len,embedding_size,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.pos=np.arange(max_sentence_len).reshape(1,-1).T\n",
        "        self.i=np.arange(embedding_size/2).reshape(1,-1)\n",
        "        self.pos_emb=np.empty((1,max_sentence_len,embedding_size))\n",
        "        self.pos_emb[:,:,0 : :2]=np.sin(self.pos / np.power(10000, (2 * self.i / embedding_size)))\n",
        "        self.pos_emb[:,:,1 : :2]=np.cos(self.pos / np.power(10000, (2 * self.i / embedding_size)))\n",
        "        self.positional_embedding = tf.cast(self.pos_emb,dtype=tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.positional_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzVaaC3JY10g"
      },
      "outputs": [],
      "source": [
        "#The paddding_mask class is designed to create a mask for padding tokens in a sequence.\n",
        "# This mask is used to prevent the model from paying attention to padding tokens during training and inference.\n",
        "class paddding_mask(tf.keras.layers.Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "    def call(self,inputs):\n",
        "        # Create a mask where padding tokens (zeros) are marked with 0, and non-padding tokens are marked with 1\n",
        "        mask=1-tf.cast(tf.math.equal(inputs,0),tf.float32)\n",
        "        #expands the mask to having an additional dimension to make it compatible with the attention mechanisms\n",
        "        return mask[:, tf.newaxis, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAdX1QXSY21m"
      },
      "outputs": [],
      "source": [
        "#This class ensures that at each position in the sequence, the model can only consider the current and previous positions, not future positions.\n",
        "class create_look_ahead_mask(tf.keras.layers.Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        #we initiliaze the layer with any extra arguments\n",
        "        super().__init__(**kwargs)\n",
        "    # the function call creates a look ahead mask to prevent the model from attending to future tokens during training\n",
        "    def call(self,sequence_length):\n",
        "        mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUIWUBHoY5zv"
      },
      "outputs": [],
      "source": [
        "# Custom layer to create padding mask to ignore padding tokens during processing.\n",
        "# it converts input sequences into dense embeddings\n",
        "# applies positional encodings to the embeddings to capture positional relationships in the sequences\n",
        "class input_layer_encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,max_sentence_len,embedding_size,vocab_size,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.paddding_mask=paddding_mask()\n",
        "\n",
        "        self.embedding=tf.keras.layers.Embedding(vocab_size,\n",
        "                                                 embedding_size,\n",
        "                                                 input_length=max_sentence_len,\n",
        "                                                 input_shape=(max_sentence_len,))\n",
        "\n",
        "        self.positional_encoding=positional_encoding(max_sentence_len,embedding_size)\n",
        "    def call(self,inputs):\n",
        "        mask=self.paddding_mask(inputs)\n",
        "\n",
        "        emb=self.embedding(inputs)\n",
        "\n",
        "        emb=self.positional_encoding(emb)\n",
        "        return emb,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V60SC_08Y6tP"
      },
      "outputs": [],
      "source": [
        "# Custom layer to create padding mask\n",
        "class input_layer_decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_sentence_len, embedding_size, vocab_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Initialize the padding mask layer\n",
        "        self.paddding_mask = paddding_mask()\n",
        "\n",
        "        # Embedding layer to convert token indices to dense vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                   embedding_size,\n",
        "                                                   input_length=max_sentence_len,\n",
        "                                                   input_shape=(max_sentence_len,))\n",
        "\n",
        "        # Positional encoding layer to add positional information to embeddings\n",
        "        self.positional_encoding = positional_encoding(max_sentence_len, embedding_size)\n",
        "\n",
        "        # Look-ahead mask for the decoder self-attention\n",
        "        self.look_ahead_mask = create_look_ahead_mask()\n",
        "\n",
        "        # Maximum sentence length for the look-ahead mask\n",
        "        self.max_sentence_len = max_sentence_len\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Calculate padding mask for the input sequences\n",
        "        mask = self.paddding_mask(inputs)\n",
        "\n",
        "        # Convert input sequences to embeddings\n",
        "        emb = self.embedding(inputs)\n",
        "\n",
        "        # Apply positional encoding to the embeddings\n",
        "        emb = self.positional_encoding(emb)\n",
        "\n",
        "        # Create look-ahead mask for self-attention in decoder\n",
        "        look_ahead_mask = self.look_ahead_mask(self.max_sentence_len)\n",
        "\n",
        "        # Combine look-ahead mask and padding mask using bitwise AND operation\n",
        "        look_ahead_mask = tf.bitwise.bitwise_and(tf.cast(look_ahead_mask, dtype=tf.int8),\n",
        "                                                 tf.cast(mask, dtype=tf.int8))\n",
        "\n",
        "        # Return processed embeddings and combined mask\n",
        "        return emb, look_ahead_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxYVqREWY8zg"
      },
      "outputs": [],
      "source": [
        "class Encoder_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 embedding_size,\n",
        "                 heads_num,\n",
        "                 dense_num,\n",
        "                 dropout_rate=0.0,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Multi-head self-attention mechanism\n",
        "        self.multi_attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=heads_num,\n",
        "            key_dim=embedding_size,\n",
        "            dropout=dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.Dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "        # Feed-forward neural network (position-wise feed-forward network)\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dense_num, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(dense_num, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(dense_num, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embedding_size, activation=\"relu\"),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Residual connection followed by layer normalization\n",
        "        self.add = tf.keras.layers.Add()\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        # Multi-head self-attention\n",
        "        mha = self.multi_attention(inputs, inputs, inputs, mask=mask)\n",
        "\n",
        "        # Add and normalize the residual connection (skip connection)\n",
        "        norm1 = self.norm1(self.add([inputs, mha]))\n",
        "\n",
        "        # Feed-forward neural network\n",
        "        ff = self.ff(norm1)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        ff_drop = self.Dropout(ff, training=training)\n",
        "\n",
        "        # Add and normalize the residual connection (skip connection)\n",
        "        output = self.norm2(self.add([ff_drop, norm1]))\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmEof9FrZAtA"
      },
      "outputs": [],
      "source": [
        "#this class is responsible for processing input sequences by applying multi-head self-attention and feed-forward operations with residual connections.\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 max_sentence_len,\n",
        "                 embedding_size,\n",
        "                 vocab_size,\n",
        "                 heads_num,\n",
        "                 dense_num,\n",
        "                 num_of_encoders,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.add=tf.keras.layers.Add()\n",
        "        self.input_layer=input_layer_encoder(max_sentence_len,embedding_size,vocab_size)\n",
        "        self.encoder_layer=[Encoder_layer(embedding_size,heads_num, dense_num) for i in range (num_of_encoders)]\n",
        "        self.num_layers=num_of_encoders\n",
        "    def call(self,inputs,training):\n",
        "        emb,mask=self.input_layer(inputs)\n",
        "        skip=emb\n",
        "        for layer in self.encoder_layer:\n",
        "            emb = layer(emb, mask,training)\n",
        "            emb = self.add([skip,emb])\n",
        "            skip = emb\n",
        "        return emb,mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGiVNBoiZGj4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class decoder_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 embedding_size,\n",
        "                 heads_num,\n",
        "                 dense_num,\n",
        "                 dropout_rate=0.0,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Multi-head self-attention for masked decoder inputs\n",
        "        self.masked_mha = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=heads_num,\n",
        "            key_dim=embedding_size,\n",
        "            dropout=dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Multi-head attention for attending to encoder outputs\n",
        "        self.multi_attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=heads_num,\n",
        "            key_dim=embedding_size,\n",
        "            dropout=dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Feed-forward neural network\n",
        "        self.ff = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dense_num, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(dense_num, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(dense_num, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embedding_size, activation=\"relu\"),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Dropout layer\n",
        "        self.Dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "        # Addition layer for residual connections\n",
        "        self.add = tf.keras.layers.Add()\n",
        "\n",
        "        # Layer normalization layers\n",
        "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
        "        self.norm3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, encoder_output, enc_mask, look_head_mask, training):\n",
        "        # Masked multi-head self-attention for decoder inputs\n",
        "        mha_out, atten_score = self.masked_mha(inputs, inputs, inputs, look_head_mask, return_attention_scores=True)\n",
        "\n",
        "        # Add and normalize the residual connection\n",
        "        Q1 = self.norm1(self.add([inputs, mha_out]))\n",
        "\n",
        "        # Multi-head attention over encoder outputs\n",
        "        mha_out2, atten_score2 = self.multi_attention(Q1, encoder_output, encoder_output, enc_mask, return_attention_scores=True)\n",
        "\n",
        "        # Add and normalize the residual connection\n",
        "        Z = self.norm2(self.add([Q1, mha_out2]))\n",
        "\n",
        "        # Feed-forward network\n",
        "        fc = self.ff(Z)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        A = self.Dropout(fc, training=training)\n",
        "\n",
        "        # Add and normalize the residual connection\n",
        "        output = self.norm3(self.add([A, Z]))\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByiwmdXtZIiX"
      },
      "outputs": [],
      "source": [
        "# the class decoder encapsulates multiple layers of the class decooder layer instances within a transformer decoder stack.\n",
        "# it orchestrates the processing of input sequences throught, embedding, masking, positional encoding and multiple decoder layers\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self,\n",
        "                 max_sentence_len,\n",
        "                 embedding_size,\n",
        "                 vocab_size,\n",
        "                 heads_num,\n",
        "                 dense_num,\n",
        "                 num_of_decoders,\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.add=tf.keras.layers.Add()\n",
        "        self.input_layer=input_layer_decoder(max_sentence_len,embedding_size,vocab_size)\n",
        "        self.decoder_layer=[decoder_layer(embedding_size,heads_num, dense_num) for i in range (num_of_decoders)]\n",
        "        self.num_layers=num_of_decoders\n",
        "    def call(self,inputs,encoder_output,enc_mask,training):\n",
        "        emb,look_head_mask=self.input_layer(inputs)\n",
        "        skip=emb\n",
        "        for layer in self.decoder_layer:\n",
        "            emb = layer(emb,encoder_output,enc_mask,look_head_mask,training)\n",
        "            emb = self.add([skip,emb])\n",
        "            skip = emb\n",
        "        return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZqODuWqZMG4"
      },
      "outputs": [],
      "source": [
        "#this class integrates the encoder and decoder to implement a transformer\n",
        "\n",
        "class transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 max_sentence_len_1=None,\n",
        "                 max_sentence_len_2=None,\n",
        "                 embedding_size=None,\n",
        "                 vocab_size1=None,\n",
        "                 vocab_size2=None,\n",
        "                 heads_num=None,\n",
        "                 dense_num=None,\n",
        "                 num_of_encoders_decoders=None):\n",
        "\n",
        "        super(transformer,self).__init__()\n",
        "\n",
        "        # Initialize the encoder with specified parameters\n",
        "        self.Encoder = Encoder(max_sentence_len_1,\n",
        "                               embedding_size,\n",
        "                               vocab_size1,\n",
        "                               heads_num,\n",
        "                               dense_num,\n",
        "                               num_of_encoders_decoders)\n",
        "\n",
        "        # Initialize the decoder with specified parameters\n",
        "        self.Decoder = Decoder(max_sentence_len_2,\n",
        "                               embedding_size,\n",
        "                               vocab_size2,\n",
        "                               heads_num,\n",
        "                               dense_num,\n",
        "                               num_of_encoders_decoders)\n",
        "\n",
        "        # Final dense layer for transforming decoder outputs to vocabulary size\n",
        "        self.Final_layer = tf.keras.layers.Dense(vocab_size2, activation='relu')\n",
        "\n",
        "        # Softmax activation to generate probabilities over the vocabulary\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Unpack input sequences\n",
        "        input_sentence, output_sentence = inputs\n",
        "\n",
        "        # Encode the input sentence to get encoder output and mask\n",
        "        enc_output, enc_mask = self.Encoder(input_sentence)\n",
        "\n",
        "        # Decode using the output sentence, encoder output, and encoder mask\n",
        "        dec_output = self.Decoder(output_sentence, enc_output, enc_mask)\n",
        "\n",
        "        # Apply final dense layer\n",
        "        final_out = self.Final_layer(dec_output)\n",
        "\n",
        "        # Apply softmax to get the final probabilities over the vocabulary\n",
        "        softmax_out = self.softmax(final_out)\n",
        "\n",
        "        return softmax_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6el4GNRZPiY"
      },
      "outputs": [],
      "source": [
        "# here I initialized an instance of the transformer class with specific parameters\n",
        "#after initialization we can use this for training, evaluation etc\n",
        "tran=transformer(max_sentence_len_1=14, #max length of input sequences\n",
        "                     max_sentence_len_2=13, #max length of output sequences\n",
        "                     embedding_size=300, #dimensionality of the embedding vectors used in encoder and decoder layer\n",
        "                     vocab_size1=german_vocab_size+1, #size of vocabulary for the input language(German)\n",
        "                     vocab_size2=english_vocab_size+1, #size of vocabulary for the input language(English)\n",
        "                     heads_num=5, #number of attention heads in each encoder and decoder layer\n",
        "                     dense_num=512, #number of units in the feed forward neural network layers, within each encoder and decoder\n",
        "                     num_of_encoders_decoders=2) #number of encoder and decoder layers stackedin the transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp-I4-13ZTd3",
        "outputId": "c4078451-5835-40ad-a6d3-794872eb36bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder (Encoder)           multiple                  109471948 \n",
            "                                                                 \n",
            " decoder (Decoder)           multiple                  43907248  \n",
            "                                                                 \n",
            " dense_16 (Dense)            multiple                  35134526  \n",
            "                                                                 \n",
            " softmax (Softmax)           multiple                  0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 188513722 (719.12 MB)\n",
            "Trainable params: 188513722 (719.12 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tran((preproc_german_sentences[:1],preproc_english_sentences[:1,:-1]))\n",
        "tran.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00p1FgaUJ4TJ"
      },
      "source": [
        "As we can see here we have the total number of parameters which is 188513722 it is very large and also has a memory size of 719.12 MB.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GSNDSZYZt6P"
      },
      "outputs": [],
      "source": [
        "#here the \"compile\" prepares the model for trainingn by specifying the loss function, optimizer and metrics\n",
        "tran.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "             optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "             metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cV8cmmgFZvxH",
        "outputId": "b5e623fb-5fe7-4821-dd32-84eccc1163c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30004/30004 [==============================] - 7775s 259ms/step - loss: 6.2373 - accuracy: 0.3766\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7836ed0ed960>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#here I start training the model, input data is the preproc_german_sentences, preproc_english_sentences and target data is \"preproc_english_sentences\"\n",
        "tran.fit((preproc_german_sentences,preproc_english_sentences[:,:-1]),\n",
        "         preproc_english_sentences[:,1:,tf.newaxis],\n",
        "         epochs=1, verbose = True,\n",
        "         batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as3eGrRzmADz"
      },
      "outputs": [],
      "source": [
        "#the prepare_pred function is intialized to prepare the input sequences for prediction using a transformer model.\n",
        "def prepare_pred(sent):\n",
        "    output=english_tokenizer.texts_to_sequences(sent)\n",
        "    output=pad(output,13)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0iJmxrS4c1M"
      },
      "outputs": [],
      "source": [
        "#this function is designed to generate predictions using the \"tran\" transfomer model that I have created earlier\n",
        "def pred(i):\n",
        "    sent = [\"<SOS>\"]  # Initialize the output sentence with a start token\n",
        "    german_token = prepare_pred(sent)  # Prepare the initial input token for the decoder\n",
        "\n",
        "    # Predict the next word iteratively until the end-of-sequence token is predicted or 12 words are generated\n",
        "    for j in range(12):\n",
        "        # Prepare the current input token for the decoder\n",
        "        german_token = prepare_pred(sent)\n",
        "\n",
        "        # Predict the next word using the transformer model\n",
        "        word = np.argmax(tran.predict((preproc_german_sentences[[i]], german_token), verbose=0), -1)[0, j]\n",
        "\n",
        "        # Convert the predicted word index back to text and append it to the output sentence\n",
        "        sent[0] = sent[0] + \" \" + english_tokenizer.sequences_to_texts(np.array([[word]]))[0]\n",
        "\n",
        "        # Break the loop if the end-of-sequence token is predicted\n",
        "        if english_tokenizer.sequences_to_texts(np.array([[word]]))[0] == \"eos\":\n",
        "            break\n",
        "\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIVombtp5Jl3",
        "outputId": "63eee71c-0053-4656-e2b2-20e32500cacd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "german sent :  ['herkunft europ√§ische union sicherheit genug sein doch im moment ist sie es nicht eos']\n",
            "predict sent :  <SOS> european union is not however the european union is not the moment\n",
            "true sent :  eu origin may be assurance enough but it is not at this moment eos\n",
            "BLEU score: 4.90260194222537e-155\n",
            "----------------\n",
            "german sent :  ['und zwar mit abschreckenden strafen belegt werden sondern auch der besitz von kinderpornographie eos']\n",
            "predict sent :  <SOS> and with the other people are also being the part of all\n",
            "true sent :  deterrent but that the possession of child pornography itself should also be punishable eos\n",
            "BLEU score: 1.090462944153118e-231\n",
            "----------------\n",
            "german sent :  ['sehr gr√ºndliche diskussion √ºber die rechtsgrundlage des berichts von frau schleicher gef√ºhrt haben eos']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predict sent :  <SOS> i have had a debate on the legal basis of report eos\n",
            "true sent :  debate took place on mr schleicher's report in the committee on constitutional affairs eos\n",
            "BLEU score: 1.2390051155620427e-231\n",
            "----------------\n",
            "german sent :  ['union vor √ºbergriffen und eingriffen europ√§ischer institutionen in die grundrechte der b√ºrger sch√ºtzen eos']\n",
            "predict sent :  <SOS> the european union and european institutions and european institutions eos\n",
            "true sent :  interference and intervention of european institutions in the fundamental rights of the citizens eos\n",
            "BLEU score: 4.854408244229234e-155\n",
            "----------------\n",
            "german sent :  ['3 aufgrund seines diskriminierenden inhalts im hinblick auf die religionszugeh√∂rigkeit √ºberhaupt zul√§ssig ist eos']\n",
            "predict sent :  <SOS> the discriminatory discriminatory access to the discriminatory discriminatory access to health eos\n",
            "true sent :  been checked for admissibility as it is wholly discriminatory on grounds of religion eos\n",
            "BLEU score: 9.853445011990208e-232\n",
            "----------------\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def show():\n",
        "    i = random.randint(0,170111)\n",
        "\n",
        "    print(\"german sent : \", german_tokenizer.sequences_to_texts(preproc_german_sentences[[i]]))\n",
        "\n",
        "    # Assuming that pred(i) returns a list with a single string element\n",
        "    predict_sent = pred(i)[0]\n",
        "    print(\"predict sent : \", predict_sent)\n",
        "\n",
        "    # Assuming that english_tokenizer.sequences_to_texts(preproc_english_sentences[[i]]) returns a list with a single string element\n",
        "    true_sent = english_tokenizer.sequences_to_texts(preproc_english_sentences[[i]])[0]\n",
        "    print(\"true sent : \", true_sent)\n",
        "\n",
        "    # Since predict_sent and true_sent are now strings, splitting should work\n",
        "    predict_sent_words = predict_sent.split(' ')\n",
        "    true_sent_words = true_sent.split(' ')\n",
        "\n",
        "    # If your predicted sentence starts with '<SOS> ', you might want to remove it\n",
        "    if predict_sent_words[0] == '<SOS>':\n",
        "        predict_sent_words = predict_sent_words[1:]\n",
        "\n",
        "    # Calculate the BLEU score\n",
        "    bleu_score = sentence_bleu([true_sent_words], predict_sent_words)\n",
        "    print('BLEU score: {}'.format(bleu_score))\n",
        "\n",
        "\n",
        "# Call the function\n",
        "for i in range(5):\n",
        "    show()\n",
        "    print(\"----------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YYRtskdJ4TJ"
      },
      "source": [
        "REMARKS:\n",
        "\n",
        "This code trains the model using transformers. <br>\n",
        "As we can see the Bleu Score is extremly low which shows that the translation is very bad. I have also showed the german word, the translated word and the excact word that shows how should it be translated. <br>\n",
        "The result of the bleu score is very low because we have a very large dataset as I have showed in the upper part of the code with 51722761 English words and 48454703 German words, and I used only 30.000 sentences to train on. The reason for this was that it was taking a very large amount of time and the computational resources that I use are not that efficient for large dataset.<br>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
